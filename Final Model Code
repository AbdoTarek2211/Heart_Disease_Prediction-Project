#Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#Scikit Learn Libraries To Evaluate and Train The Model
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error
from sklearn.feature_selection import SelectKBest, f_classif

#reading and preprocessing
df = pd.read_csv("/content/sample_data/Heart_Disease.csv")
df = df.fillna(df.mean(numeric_only=True))
df = df.dropna()
df = df.drop_duplicates()
df.isna().sum()

# Get the number of rows and columns
num_rows, num_cols = df.shape
print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_cols}")
x = df["Age"]
y = df["Cholesterol"]
plt.scatter(x, y)
plt.xlabel("Age")
plt.ylabel("Cholesterol Levels")
plt.title("Scatter Plot of Age vs. Cholesterol Levels")
plt.show()

#One-hot Encoding using get_dummies
df = pd.get_dummies(df,columns=['Gender','work_type','smoking_status','HeartDisease'])

# Select the dependent and independent features
X = df.drop(['HeartDisease_Yes','HeartDisease_No'], axis=1)  # Independent features
y = df['HeartDisease_Yes']  # Dependent feature

# Univariate Feature Selection
selector = SelectKBest(f_classif,k=5)
selector.fit(X,y)
selected_columns = selector.get_support(indices=True)
X = X.iloc[:, selected_columns]

# Visualize the correlation with the outliers
corr_matrix = X.corr()

# Visualize the correlation matrix
plt.figure(figsize=(22, 16))
plt.title('Correlation Matrix')
sns.heatmap(corr_matrix, annot=True, cmap='magma')
plt.show()

# Detect and remove outliers using the IQR method
Q1 = X.quantile(0.25)
Q3 = X.quantile(0.75)
IQR = Q3 - Q1
X_outliers_removed = X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]
y_outliers_removed = y[X_outliers_removed.index]

# Visualize the correlation matrix after removing outliers
corr_matrix = X_outliers_removed.corr()
plt.figure(figsize=(25, 16))
plt.title('Correlation Matrix (After Removing Outliers)')
sns.heatmap(corr_matrix, annot=True, cmap='viridis')
plt.show()

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_outliers_removed, y_outliers_removed, test_size=0.31, random_state=40)

#Standardize the features of a dataset by subtracting the mean and dividing by the standard deviation for each feature in both the training and test sets.
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Logistic Regression Training Model
lr_model = LogisticRegression(random_state=42)
lr_model.fit(X_train, y_train)

# Logistic Regression
lr_pred = lr_model.predict(X_test)
lr_acc = accuracy_score(y_test, lr_pred)
lr_cm = confusion_matrix(y_test, lr_pred)
lr_cr = classification_report(y_test, lr_pred)
lr_mse = mean_squared_error(y_test, lr_pred)

# Print the evaluation metrics for Logistic Regresssion
print('Logistic Regression Accuracy:', lr_acc)
print('Logistic Regression Confusion Matrix:', lr_cm)
print('Logistic Regression Classification Report:', lr_cr)
print('Logistic Regression Mean Squared Error:', lr_mse)

# SVM Training Model
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(X_train, y_train)

# SVM
svm_pred = svm_model.predict(X_test)
svm_acc = accuracy_score(y_test, svm_pred)
svm_cm = confusion_matrix(y_test, svm_pred)
svm_cr = classification_report(y_test, svm_pred)
svm_mse = mean_squared_error(y_test, svm_pred)

# Print the evaluation metrics for SVM
print('SVM Accuracy:', svm_acc)
print('SVM Confusion Matrix:', svm_cm)
print('SVM Classification Report:', svm_cr)
print('SVM Mean Squared Error:', svm_mse)

# Decision Tree (ID3) Training Model
dt_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
dt_model.fit(X_train, y_train)

# Decision Tree (ID3)
dt_pred = dt_model.predict(X_test)
dt_acc = accuracy_score(y_test, dt_pred)
dt_cm = confusion_matrix(y_test, dt_pred)
dt_cr = classification_report(y_test, dt_pred)
dt_mse = mean_squared_error(y_test, dt_pred)

# Print the evaluation metrics for Decision Tree (ID3)
print('Decision Tree (ID3) Accuracy:', dt_acc)
print('Decision Tree (ID3) Confusion Matrix:', dt_cm)
print('Decision Tree (ID3) Classification Report:', dt_cr)
print('Decision Tree (ID3) Mean Squared Error:', dt_mse)

rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Train the random forest model using the training set
rf.fit(X_train, y_train)

# Use the trained model to make predictions on the test set
y_pred = rf.predict(X_test)
y_cm = confusion_matrix(y_test, y_pred)
y_cr = classification_report(y_test, y_pred)
y_mse = mean_squared_error(y_test, y_pred)
y_acc = accuracy_score(y_test, y_pred)

# Print the evaluation metrics for Logistic Regresssion
print('Random Forest Accuracy:', y_acc)
print('Random Forest Confusion Matrix:', y_cm)
print('Random Forest Classification Report:', y_cr)
print('Random Forest Mean Squared Error:', y_mse)
