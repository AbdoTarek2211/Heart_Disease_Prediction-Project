# Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import joblib
import warnings
warnings.filterwarnings('ignore')

# Scikit Learn Libraries To Evaluate and Train The Model
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, f1_score, precision_score, recall_score
from sklearn.feature_selection import SelectKBest, f_classif

# Reading and preprocessing
print("Loading and preprocessing data...")
df = pd.read_csv("/content/sample_data/Heart_Disease.csv")

# Check initial data info
print(f"Initial dataset shape: {df.shape}")
print(f"Missing values before preprocessing:\n{df.isnull().sum()}")

# Handle missing values - fill numeric columns with median instead of mean for better robustness
numeric_columns = df.select_dtypes(include=[np.number]).columns
df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())

# Fill categorical columns with mode
categorical_columns = df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    df[col] = df[col].fillna(df[col].mode()[0])

# Remove duplicates
df = df.drop_duplicates()

# Final check for missing values
print(f"Missing values after preprocessing:\n{df.isnull().sum()}")

# Get the number of rows and columns after preprocessing
num_rows, num_cols = df.shape
print(f"Final dataset shape - Rows: {num_rows}, Columns: {num_cols}")

# Basic visualization - Age vs Cholesterol scatter plot
if 'Age' in df.columns and 'Cholesterol' in df.columns:
    plt.figure(figsize=(10, 6))
    x = df["Age"]
    y = df["Cholesterol"]
    plt.scatter(x, y, alpha=0.6)
    plt.xlabel("Age")
    plt.ylabel("Cholesterol Levels")
    plt.title("Scatter Plot of Age vs. Cholesterol Levels")
    plt.grid(True, alpha=0.3)
    plt.show()

# One-hot Encoding using get_dummies - only for categorical columns
categorical_cols_to_encode = []
for col in df.columns:
    if df[col].dtype == 'object' or col in ['Gender', 'work_type', 'smoking_status']:
        categorical_cols_to_encode.append(col)

# Handle target variable separately
if 'HeartDisease' in categorical_cols_to_encode:
    categorical_cols_to_encode.remove('HeartDisease')

print(f"Encoding categorical columns: {categorical_cols_to_encode}")

if categorical_cols_to_encode:
    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=False)

# Handle target variable encoding
if 'HeartDisease' in df.columns:
    df = pd.get_dummies(df, columns=['HeartDisease'], drop_first=False)
    target_col = 'HeartDisease_Yes' if 'HeartDisease_Yes' in df.columns else 'HeartDisease_1'
else:
    # Assume the target is already encoded or named differently
    target_cols = [col for col in df.columns if 'heart' in col.lower() or 'disease' in col.lower()]
    if target_cols:
        target_col = target_cols[0]
    else:
        print("Warning: Could not identify target column. Please check your data.")

print(f"Target column identified: {target_col}")

# Select the dependent and independent features
target_columns = [col for col in df.columns if 'HeartDisease' in col]
X = df.drop(target_columns, axis=1)  # Independent features
y = df[target_col]  # Dependent feature

print(f"Features shape: {X.shape}")
print(f"Target distribution:\n{y.value_counts()}")

# Univariate Feature Selection - select optimal number of features
selector = SelectKBest(f_classif, k=min(10, X.shape[1]))  # Select up to 10 best features
selector.fit(X, y)
selected_features = selector.get_support(indices=True)
selected_feature_names = X.columns[selected_features]
X_selected = X.iloc[:, selected_features]

print(f"Selected {len(selected_features)} best features: {list(selected_feature_names)}")

# Visualize the correlation matrix of selected features
plt.figure(figsize=(12, 10))
corr_matrix = X_selected.corr()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Show only lower triangle
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, mask=mask, cbar_kws={"shrink": .8})
plt.title('Correlation Matrix of Selected Features')
plt.tight_layout()
plt.show()

# Detect and remove outliers using the IQR method
print("Removing outliers using IQR method...")
Q1 = X_selected.quantile(0.25)
Q3 = X_selected.quantile(0.75)
IQR = Q3 - Q1

# Create outlier mask
outlier_mask = ~((X_selected < (Q1 - 1.5 * IQR)) | (X_selected > (Q3 + 1.5 * IQR))).any(axis=1)
X_clean = X_selected[outlier_mask]
y_clean = y[outlier_mask]

print(f"Removed {len(X_selected) - len(X_clean)} outliers")
print(f"Clean dataset shape: {X_clean.shape}")

# Visualize the correlation matrix after removing outliers
plt.figure(figsize=(12, 10))
corr_matrix_clean = X_clean.corr()
mask = np.triu(np.ones_like(corr_matrix_clean, dtype=bool))
sns.heatmap(corr_matrix_clean, annot=True, cmap='viridis', center=0,
            square=True, mask=mask, cbar_kws={"shrink": .8})
plt.title('Correlation Matrix (After Removing Outliers)')
plt.tight_layout()
plt.show()

# Split into training and testing sets with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y_clean, test_size=0.3, random_state=42, stratify=y_clean
)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize results storage
results = {}
models = {}

print("\n" + "="*50)
print("TRAINING AND EVALUATING MODELS")
print("="*50)

# 1. Logistic Regression
print("\n1. Training Logistic Regression...")
lr_model = LogisticRegression(random_state=42, max_iter=1000)
lr_model.fit(X_train_scaled, y_train)

lr_pred = lr_model.predict(X_test_scaled)
lr_acc = accuracy_score(y_test, lr_pred)
lr_f1 = f1_score(y_test, lr_pred)
lr_precision = precision_score(y_test, lr_pred)
lr_recall = recall_score(y_test, lr_pred)
lr_cm = confusion_matrix(y_test, lr_pred)
lr_cv_score = cross_val_score(lr_model, X_train_scaled, y_train, cv=5).mean()

models['Logistic Regression'] = lr_model
results['Logistic Regression'] = {
    'Accuracy': lr_acc,
    'F1-Score': lr_f1,
    'Precision': lr_precision,
    'Recall': lr_recall,
    'CV Score': lr_cv_score,
    'Confusion Matrix': lr_cm
}

print(f'Logistic Regression Results:')
print(f'Accuracy: {lr_acc:.4f}')
print(f'F1-Score: {lr_f1:.4f}')
print(f'Cross-validation Score: {lr_cv_score:.4f}')
print(f'Confusion Matrix:\n{lr_cm}')

# 2. Support Vector Machine
print("\n2. Training Support Vector Machine...")
svm_model = SVC(kernel='rbf', random_state=42, probability=True)  # Changed to rbf kernel
svm_model.fit(X_train_scaled, y_train)

svm_pred = svm_model.predict(X_test_scaled)
svm_acc = accuracy_score(y_test, svm_pred)
svm_f1 = f1_score(y_test, svm_pred)
svm_precision = precision_score(y_test, svm_pred)
svm_recall = recall_score(y_test, svm_pred)
svm_cm = confusion_matrix(y_test, svm_pred)
svm_cv_score = cross_val_score(svm_model, X_train_scaled, y_train, cv=5).mean()

models['SVM'] = svm_model
results['SVM'] = {
    'Accuracy': svm_acc,
    'F1-Score': svm_f1,
    'Precision': svm_precision,
    'Recall': svm_recall,
    'CV Score': svm_cv_score,
    'Confusion Matrix': svm_cm
}

print(f'SVM Results:')
print(f'Accuracy: {svm_acc:.4f}')
print(f'F1-Score: {svm_f1:.4f}')
print(f'Cross-validation Score: {svm_cv_score:.4f}')
print(f'Confusion Matrix:\n{svm_cm}')

# 3. Decision Tree
print("\n3. Training Decision Tree...")
dt_model = DecisionTreeClassifier(
    criterion='entropy', 
    random_state=42, 
    max_depth=10,  # Added max_depth to prevent overfitting
    min_samples_split=20,
    min_samples_leaf=10
)
dt_model.fit(X_train_scaled, y_train)

dt_pred = dt_model.predict(X_test_scaled)
dt_acc = accuracy_score(y_test, dt_pred)
dt_f1 = f1_score(y_test, dt_pred)
dt_precision = precision_score(y_test, dt_pred)
dt_recall = recall_score(y_test, dt_pred)
dt_cm = confusion_matrix(y_test, dt_pred)
dt_cv_score = cross_val_score(dt_model, X_train_scaled, y_train, cv=5).mean()

models['Decision Tree'] = dt_model
results['Decision Tree'] = {
    'Accuracy': dt_acc,
    'F1-Score': dt_f1,
    'Precision': dt_precision,
    'Recall': dt_recall,
    'CV Score': dt_cv_score,
    'Confusion Matrix': dt_cm
}

print(f'Decision Tree Results:')
print(f'Accuracy: {dt_acc:.4f}')
print(f'F1-Score: {dt_f1:.4f}')
print(f'Cross-validation Score: {dt_cv_score:.4f}')
print(f'Confusion Matrix:\n{dt_cm}')

# 4. Random Forest
print("\n4. Training Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=100, 
    max_depth=15,  # Slightly increased max_depth
    random_state=42,
    min_samples_split=10,
    min_samples_leaf=5,
    bootstrap=True
)
rf_model.fit(X_train_scaled, y_train)

rf_pred = rf_model.predict(X_test_scaled)
rf_acc = accuracy_score(y_test, rf_pred)
rf_f1 = f1_score(y_test, rf_pred)
rf_precision = precision_score(y_test, rf_pred)
rf_recall = recall_score(y_test, rf_pred)
rf_cm = confusion_matrix(y_test, rf_pred)
rf_cv_score = cross_val_score(rf_model, X_train_scaled, y_train, cv=5).mean()

models['Random Forest'] = rf_model
results['Random Forest'] = {
    'Accuracy': rf_acc,
    'F1-Score': rf_f1,
    'Precision': rf_precision,
    'Recall': rf_recall,
    'CV Score': rf_cv_score,
    'Confusion Matrix': rf_cm
}

print(f'Random Forest Results:')
print(f'Accuracy: {rf_acc:.4f}')
print(f'F1-Score: {rf_f1:.4f}')
print(f'Cross-validation Score: {rf_cv_score:.4f}')
print(f'Confusion Matrix:\n{rf_cm}')

# Compare all models and find the best one
print("\n" + "="*50)
print("MODEL COMPARISON")
print("="*50)

comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[model]['Accuracy'] for model in results.keys()],
    'F1-Score': [results[model]['F1-Score'] for model in results.keys()],
    'Precision': [results[model]['Precision'] for model in results.keys()],
    'Recall': [results[model]['Recall'] for model in results.keys()],
    'CV Score': [results[model]['CV Score'] for model in results.keys()]
})

print(comparison_df.round(4))

# Find best model based on F1-score (good balance of precision and recall)
best_model_name = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'Model']
best_model = models[best_model_name]
best_f1_score = comparison_df.loc[comparison_df['F1-Score'].idxmax(), 'F1-Score']

print(f"\nBest Model: {best_model_name}")
print(f"Best F1-Score: {best_f1_score:.4f}")

# Visualize model comparison
plt.figure(figsize=(12, 8))
metrics = ['Accuracy', 'F1-Score', 'Precision', 'Recall', 'CV Score']
x = np.arange(len(comparison_df))
width = 0.15

for i, metric in enumerate(metrics):
    plt.bar(x + i * width, comparison_df[metric], width, label=metric, alpha=0.8)

plt.xlabel('Models')
plt.ylabel('Score')
plt.title('Model Performance Comparison')
plt.xticks(x + width * 2, comparison_df['Model'], rotation=45)
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Save the best model and preprocessing components
print(f"\nSaving the best model ({best_model_name}) and preprocessing components...")

# Create a model package with all necessary components
model_package = {
    'model': best_model,
    'scaler': scaler,
    'feature_selector': selector,
    'selected_features': selected_feature_names.tolist(),
    'model_name': best_model_name,
    'performance_metrics': results[best_model_name],
    'feature_names': list(X.columns)
}

# Save the complete model package
joblib.dump(model_package, 'best_heart_disease_model.pkl')
print("Model package saved as 'best_heart_disease_model.pkl'")

# Save individual components for flexibility
joblib.dump(best_model, f'best_model_{best_model_name.lower().replace(" ", "_")}.pkl')
joblib.dump(scaler, 'feature_scaler.pkl')
joblib.dump(selector, 'feature_selector.pkl')

print("Individual components saved:")
print(f"- best_model_{best_model_name.lower().replace(' ', '_')}.pkl")
print("- feature_scaler.pkl")
print("- feature_selector.pkl")

# Create a simple prediction function demonstration
print("\n" + "="*50)
print("PREDICTION EXAMPLE")
print("="*50)

# Example prediction using the best model
sample_index = 0
sample_data = X_test.iloc[[sample_index]]
sample_data_scaled = scaler.transform(sample_data)
prediction = best_model.predict(sample_data_scaled)[0]
prediction_proba = None

# Get prediction probability if available
if hasattr(best_model, 'predict_proba'):
    prediction_proba = best_model.predict_proba(sample_data_scaled)[0].max()

print(f"Sample prediction using {best_model_name}:")
print(f"Input features: {sample_data.values[0]}")
print(f"Predicted class: {prediction}")
if prediction_proba:
    print(f"Prediction confidence: {prediction_proba:.4f}")
print(f"Actual class: {y_test.iloc[sample_index]}")

print("\n" + "="*50)
print("MODEL TRAINING COMPLETED SUCCESSFULLY!")
print("="*50)
